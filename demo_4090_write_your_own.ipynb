{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Your Own GPU Kernel, `torch.compile`, and Data Types\n",
    "\n",
    "The [beginner notebook](demo_4090.ipynb) showed how to download pre-compiled kernels. The [advanced notebook](demo_4090_advanced.ipynb) showed kernel fusion and model patching. This notebook goes one step further:\n",
    "\n",
    "1. **Write your own GPU kernel from scratch** using Triton — a Python-like language for GPU programming\n",
    "2. **`torch.compile`** — PyTorch's built-in compiler that auto-fuses operations (no manual kernel writing needed)\n",
    "3. **Four-way race** — compare PyTorch eager, your Triton kernel, Hub kernel, and `torch.compile` head-to-head\n",
    "4. **Data types** — how fp32, fp16, and bf16 affect performance and when to use each\n",
    "5. **Memory footprint** — how fusion reduces peak GPU memory, not just speed\n",
    "6. **Compile a full model** — apply `torch.compile` to a complete transformer block and compare with manual kernel patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import triton\n",
    "import triton.language as tl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from kernels import get_kernel\n",
    "\n",
    "act_kernel = get_kernel(\"kernels-community/activation\")\n",
    "norm_kernel = get_kernel(\"kernels-community/triton-layer-norm\")\n",
    "\n",
    "print(f\"GPU:    {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"Torch:  {torch.__version__}\")\n",
    "print(f\"Triton: {triton.__version__}\")\n",
    "\n",
    "\n",
    "def benchmark(func, iterations=300, warmup=30):\n",
    "    \"\"\"Benchmark a GPU function. Returns average time in milliseconds.\"\"\"\n",
    "    for _ in range(warmup):\n",
    "        func()\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        func()\n",
    "    torch.cuda.synchronize()\n",
    "    return (time.perf_counter() - start) / iterations * 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Writing a GPU Kernel From Scratch with Triton\n",
    "\n",
    "### What is Triton?\n",
    "\n",
    "[Triton](https://triton-lang.org/) is a language and compiler for writing GPU kernels in Python. Instead of writing raw CUDA C++ (hundreds of lines, easy to get wrong), you write Python-like code and Triton compiles it to optimized GPU assembly.\n",
    "\n",
    "**The tradeoff:**\n",
    "- **CUDA C++**: Maximum control, maximum complexity. You manage every thread, every memory access.\n",
    "- **Triton**: ~90% of the performance, ~10% of the complexity. Triton handles thread management and memory coalescing for you.\n",
    "- **torch.compile**: Zero manual work, but less control. PyTorch's compiler generates Triton kernels automatically.\n",
    "\n",
    "### Our first kernel: Fused SiLU + Multiply\n",
    "\n",
    "We'll write the same fused `SiLU(gate) * up` operation from the advanced notebook, but this time as our own Triton kernel.\n",
    "\n",
    "Here's the complete kernel — 15 lines of actual code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def silu_mul_kernel(\n",
    "    # Pointers to the input and output tensors in GPU memory\n",
    "    gate_ptr, up_ptr, out_ptr,\n",
    "    # Total number of elements to process\n",
    "    n_elements,\n",
    "    # How many elements each \"program\" (thread block) processes\n",
    "    BLOCK_SIZE: tl.constexpr,\n",
    "):\n",
    "    # Which block am I? (like a thread ID, but for blocks of elements)\n",
    "    pid = tl.program_id(0)\n",
    "\n",
    "    # Calculate which elements this block handles\n",
    "    # Example: if BLOCK_SIZE=1024, block 0 handles elements 0-1023,\n",
    "    #          block 1 handles 1024-2047, etc.\n",
    "    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
    "\n",
    "    # Don't read past the end of the array\n",
    "    mask = offsets < n_elements\n",
    "\n",
    "    # Load gate and up values from GPU memory\n",
    "    # Cast gate to fp32 for the sigmoid computation (more accurate)\n",
    "    gate = tl.load(gate_ptr + offsets, mask=mask).to(tl.float32)\n",
    "    up = tl.load(up_ptr + offsets, mask=mask)\n",
    "\n",
    "    # SiLU(x) = x * sigmoid(x)\n",
    "    # This is the actual math — computed in fast on-chip registers,\n",
    "    # no intermediate tensor written to slow global memory!\n",
    "    silu = gate * tl.sigmoid(gate)\n",
    "\n",
    "    # Multiply by 'up' and store result\n",
    "    result = silu.to(up.dtype) * up\n",
    "    tl.store(out_ptr + offsets, result, mask=mask)\n",
    "\n",
    "\n",
    "print(\"Kernel defined! Key concepts:\")\n",
    "print(\"  @triton.jit    — compiles this Python function into GPU machine code\")\n",
    "print(\"  tl.program_id  — which block of elements am I processing?\")\n",
    "print(\"  tl.load/store  — read from / write to GPU memory\")\n",
    "print(\"  tl.constexpr   — compile-time constant (Triton optimizes for this value)\")\n",
    "print(\"  mask           — boundary check (don't read past end of array)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Python wrapper\n",
    "\n",
    "A Triton kernel operates on raw memory pointers. We need a thin Python wrapper that:\n",
    "1. Allocates the output tensor\n",
    "2. Calculates the grid size (how many blocks to launch)\n",
    "3. Calls the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triton_silu_mul(gate: torch.Tensor, up: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Fused SiLU(gate) * up using our custom Triton kernel.\"\"\"\n",
    "    assert gate.shape == up.shape\n",
    "    assert gate.is_cuda\n",
    "\n",
    "    out = torch.empty_like(gate)\n",
    "    n = gate.numel()\n",
    "\n",
    "    # Grid: how many blocks to launch?\n",
    "    # Each block processes BLOCK_SIZE elements, so we need ceil(n / BLOCK_SIZE) blocks\n",
    "    grid = lambda meta: (triton.cdiv(n, meta['BLOCK_SIZE']),)\n",
    "\n",
    "    # Launch the kernel on the GPU\n",
    "    # Triton compiles it to GPU assembly the first time, then caches it\n",
    "    silu_mul_kernel[grid](gate, up, out, n, BLOCK_SIZE=1024)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# Test it!\n",
    "gate = torch.randn((4096, 11008), dtype=torch.float16, device=\"cuda\")\n",
    "up = torch.randn((4096, 11008), dtype=torch.float16, device=\"cuda\")\n",
    "\n",
    "our_result = triton_silu_mul(gate, up)\n",
    "pytorch_result = torch.nn.functional.silu(gate) * up\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "diff = (our_result - pytorch_result).abs().max().item()\n",
    "print(f\"Shape: {gate.shape}\")\n",
    "print(f\"Max difference: {diff:.8f}\")\n",
    "print(f\"Correct: {'PASS' if diff < 0.01 else 'FAIL'}\")\n",
    "print()\n",
    "print(f\"We just wrote a GPU kernel in Python and it produced the same\")\n",
    "print(f\"result as PyTorch, processing {gate.numel():,} numbers in parallel.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Four-Way Race\n",
    "\n",
    "Now the moment of truth. We have four ways to compute `SiLU(gate) * up`:\n",
    "\n",
    "| Approach | How it works | Effort |\n",
    "|----------|-------------|--------|\n",
    "| **PyTorch eager** | Two separate ops, two memory round-trips | Zero (default) |\n",
    "| **Our Triton kernel** | Fused, one memory round-trip | ~15 lines of Triton |\n",
    "| **Hub kernel** | Pre-compiled fused kernel from HuggingFace | One line (`get_kernel`) |\n",
    "| **torch.compile** | PyTorch auto-generates a fused Triton kernel | One decorator (`@torch.compile`) |\n",
    "\n",
    "Let's race them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare torch.compile version\n",
    "@torch.compile\n",
    "def compiled_silu_mul(gate, up):\n",
    "    return torch.nn.functional.silu(gate) * up\n",
    "\n",
    "# Warm up torch.compile (first call triggers compilation)\n",
    "for _ in range(5):\n",
    "    compiled_silu_mul(gate, up)\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# Prepare Hub kernel version\n",
    "gate_up_cat = torch.cat([gate, up], dim=-1).contiguous()\n",
    "out_hub = torch.empty_like(gate)\n",
    "\n",
    "# Race!\n",
    "results = {}\n",
    "\n",
    "results['PyTorch eager'] = benchmark(\n",
    "    lambda: torch.nn.functional.silu(gate) * up\n",
    ")\n",
    "results['Our Triton kernel'] = benchmark(\n",
    "    lambda: triton_silu_mul(gate, up)\n",
    ")\n",
    "results['Hub kernel'] = benchmark(\n",
    "    lambda: act_kernel.silu_and_mul(out_hub, gate_up_cat)\n",
    ")\n",
    "results['torch.compile'] = benchmark(\n",
    "    lambda: compiled_silu_mul(gate, up)\n",
    ")\n",
    "\n",
    "eager_ms = results['PyTorch eager']\n",
    "print(f\"{'Approach':<22} {'Time (ms)':>10} {'Speedup':>8}\")\n",
    "print(\"=\" * 44)\n",
    "for name, ms in sorted(results.items(), key=lambda x: x[1]):\n",
    "    print(f\"{name:<22} {ms:>8.4f}   {eager_ms/ms:>6.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "names = list(results.keys())\n",
    "times = [results[n] for n in names]\n",
    "speedups = [eager_ms / t for t in times]\n",
    "colors = ['#FF9800', '#2196F3', '#4CAF50', '#9C27B0']\n",
    "\n",
    "bars = ax.bar(range(len(names)), times, color=colors, edgecolor='white')\n",
    "ax.set_xticks(range(len(names)))\n",
    "ax.set_xticklabels(names, rotation=15, ha='right')\n",
    "ax.set_ylabel('Time (ms) — lower is better')\n",
    "ax.set_title('SiLU + Multiply: Four-Way Race (4096 x 11008, float16)')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, ms, speedup in zip(bars, times, speedups):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
    "            f'{ms:.3f}ms\\n({speedup:.2f}x)', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"All three optimized approaches hit roughly the same speed.\")\n",
    "print(\"That's because they all do the same thing: fuse two operations into one\")\n",
    "print(\"memory round-trip. The memory bandwidth ceiling is the same for all of them.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why are the three optimized approaches nearly identical?\n",
    "\n",
    "They all solve the same problem the same way:\n",
    "1. Read `gate` and `up` from memory (one trip)\n",
    "2. Compute `SiLU(gate) * up` in fast on-chip registers\n",
    "3. Write the result back (one trip)\n",
    "\n",
    "The memory bandwidth of the 4090 (1,008 GB/s) is the hard ceiling. Once you eliminate the intermediate write (which all three approaches do), you've hit the wall. No amount of cleverness can make the GPU's memory bus faster.\n",
    "\n",
    "This is a key insight: **for memory-bound operations, the optimal kernel performance converges**. The difference between approaches is in ergonomics, not speed:\n",
    "\n",
    "- **torch.compile**: Easiest — just add a decorator. But opaque, and you can't customize the fusion strategy.\n",
    "- **Hub kernels**: One-liner to use, battle-tested, but limited to what's published.\n",
    "- **Custom Triton**: Full control — you can fuse arbitrary operations, add custom logic, target specific shapes. More work, but sometimes it's the only option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Writing a More Complex Kernel — Fused RMSNorm\n",
    "\n",
    "The SiLU+mul kernel was simple because every element is independent — element `i` of the output depends only on element `i` of the inputs. RMSNorm is harder because it requires a **reduction**: you need the mean of *all* elements in a row to normalize *each* element.\n",
    "\n",
    "$$\\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\text{mean}(x^2) + \\epsilon}} \\cdot \\gamma$$\n",
    "\n",
    "This means a single thread block must process an entire row, computing the sum of squares, then normalizing every element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def rmsnorm_kernel(\n",
    "    x_ptr, weight_ptr, out_ptr,\n",
    "    n_rows, n_cols,\n",
    "    eps,\n",
    "    BLOCK_SIZE: tl.constexpr,\n",
    "):\n",
    "    # Each program handles one row\n",
    "    row_idx = tl.program_id(0)\n",
    "    row_start = row_idx * n_cols\n",
    "\n",
    "    # --- Pass 1: Compute sum of squares for this row ---\n",
    "    sum_sq = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n",
    "    for col_offset in range(0, n_cols, BLOCK_SIZE):\n",
    "        cols = col_offset + tl.arange(0, BLOCK_SIZE)\n",
    "        mask = cols < n_cols\n",
    "        x = tl.load(x_ptr + row_start + cols, mask=mask, other=0.0).to(tl.float32)\n",
    "        sum_sq += x * x\n",
    "\n",
    "    # Mean of squares across all columns\n",
    "    mean_sq = tl.sum(sum_sq) / n_cols\n",
    "    # Reciprocal square root: 1 / sqrt(mean(x^2) + eps)\n",
    "    rrms = 1.0 / tl.sqrt(mean_sq + eps)\n",
    "\n",
    "    # --- Pass 2: Normalize and scale ---\n",
    "    for col_offset in range(0, n_cols, BLOCK_SIZE):\n",
    "        cols = col_offset + tl.arange(0, BLOCK_SIZE)\n",
    "        mask = cols < n_cols\n",
    "        x = tl.load(x_ptr + row_start + cols, mask=mask, other=0.0).to(tl.float32)\n",
    "        w = tl.load(weight_ptr + cols, mask=mask, other=0.0).to(tl.float32)\n",
    "        normed = x * rrms * w\n",
    "        tl.store(out_ptr + row_start + cols, normed.to(tl.float16), mask=mask)\n",
    "\n",
    "\n",
    "def triton_rmsnorm(x: torch.Tensor, weight: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "    \"\"\"RMSNorm using our custom Triton kernel.\"\"\"\n",
    "    assert x.ndim == 2\n",
    "    n_rows, n_cols = x.shape\n",
    "    out = torch.empty_like(x)\n",
    "    # One program per row, with BLOCK_SIZE elements per iteration\n",
    "    grid = (n_rows,)\n",
    "    rmsnorm_kernel[grid](x, weight, out, n_rows, n_cols, eps, BLOCK_SIZE=1024)\n",
    "    return out\n",
    "\n",
    "\n",
    "# Test it\n",
    "n_tokens, hidden_size = 2048, 4096\n",
    "x = torch.randn((n_tokens, hidden_size), dtype=torch.float16, device=\"cuda\")\n",
    "weight = torch.ones(hidden_size, dtype=torch.float16, device=\"cuda\")\n",
    "\n",
    "our_out = triton_rmsnorm(x, weight)\n",
    "ref_out = nn.RMSNorm(hidden_size, eps=1e-6, dtype=torch.float16, device=\"cuda\")(x)\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "diff = (our_out - ref_out).abs().max().item()\n",
    "print(f\"RMSNorm correctness: max diff = {diff:.8f} ({'PASS' if diff < 0.01 else 'FAIL'})\")\n",
    "print()\n",
    "print(\"This kernel is more complex because it needs two passes over each row:\")\n",
    "print(\"  Pass 1: Read all elements, compute sum of squares\")\n",
    "print(\"  Pass 2: Read elements again, normalize using the computed RMS, write output\")\n",
    "print(\"Each pass iterates in chunks of BLOCK_SIZE for rows wider than 1024.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark our RMSNorm against PyTorch and Hub kernel\n",
    "pytorch_rmsnorm = nn.RMSNorm(hidden_size, eps=1e-6, dtype=torch.float16, device=\"cuda\")\n",
    "\n",
    "token_counts = [512, 1024, 2048, 4096, 8192]\n",
    "\n",
    "print(f\"RMSNorm benchmark: hidden_size={hidden_size}\")\n",
    "print(f\"{'Tokens':>8} {'Our Triton':>12} {'PyTorch':>12} {'Hub':>12} {'Ours/PT':>8}\")\n",
    "print(\"-\" * 56)\n",
    "\n",
    "rmsnorm_results = []\n",
    "\n",
    "for n in token_counts:\n",
    "    x = torch.randn((n, hidden_size), dtype=torch.float16, device=\"cuda\")\n",
    "    w = torch.ones(hidden_size, dtype=torch.float16, device=\"cuda\")\n",
    "\n",
    "    ours = benchmark(lambda: triton_rmsnorm(x, w))\n",
    "    pt = benchmark(lambda: pytorch_rmsnorm(x))\n",
    "    hub = benchmark(lambda: norm_kernel.rms_norm_fn(x, w, None, eps=1e-6))\n",
    "\n",
    "    rmsnorm_results.append((n, ours, pt, hub))\n",
    "    print(f\"{n:>8} {ours:>10.4f}   {pt:>10.4f}   {hub:>10.4f}   {pt/ours:>6.2f}x\")\n",
    "\n",
    "print()\n",
    "print(\"Our simple Triton kernel is a first draft — the Hub kernel's Triton code\")\n",
    "print(\"is more sophisticated (better vectorization, shared memory usage).\")\n",
    "print(\"But even a basic Triton kernel gets you in the right ballpark.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: `torch.compile` — The Zero-Effort Alternative\n",
    "\n",
    "PyTorch 2.0 introduced `torch.compile`, which automatically:\n",
    "1. Traces your Python code to build a computation graph\n",
    "2. Identifies operations that can be fused\n",
    "3. Generates optimized Triton kernels for the fused operations\n",
    "4. Caches and reuses the compiled code\n",
    "\n",
    "It's essentially an automatic version of what we did by hand. The tradeoff:\n",
    "\n",
    "| | Manual Triton | torch.compile |\n",
    "|---|---|---|\n",
    "| **Effort** | Write kernel code | Add `@torch.compile` |\n",
    "| **First-call cost** | Near-zero | 5-30 seconds (compilation) |\n",
    "| **Control** | Full | None (compiler decides) |\n",
    "| **Custom ops** | Anything you can write | Only standard PyTorch ops |\n",
    "| **Debugging** | Straightforward | Opaque (graph breaks, dynamic shapes) |\n",
    "\n",
    "Let's test it on a full model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLaMAStyleMLP(nn.Module):\n",
    "    \"\"\"Simplified LLaMA MLP block.\"\"\"\n",
    "    def __init__(self, hidden_size=4096, intermediate_size=11008):\n",
    "        super().__init__()\n",
    "        self.norm = nn.RMSNorm(hidden_size, eps=1e-6)\n",
    "        self.gate_up_proj = nn.Linear(hidden_size, 2 * intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=False)\n",
    "        self.intermediate_size = intermediate_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.norm(x)\n",
    "        gate_up = self.gate_up_proj(x)\n",
    "        gate = gate_up[:, :self.intermediate_size]\n",
    "        up = gate_up[:, self.intermediate_size:]\n",
    "        x = torch.nn.functional.silu(gate) * up\n",
    "        x = self.down_proj(x)\n",
    "        return x + residual\n",
    "\n",
    "\n",
    "hidden_size = 4096\n",
    "intermediate_size = 11008\n",
    "model = LLaMAStyleMLP(hidden_size, intermediate_size).cuda().half()\n",
    "n_tokens = 4096\n",
    "x = torch.randn((n_tokens, hidden_size), dtype=torch.float16, device=\"cuda\")\n",
    "\n",
    "# --- Baseline: PyTorch eager ---\n",
    "with torch.no_grad():\n",
    "    eager_ms = benchmark(lambda: model(x))\n",
    "\n",
    "# --- torch.compile ---\n",
    "compiled_model = torch.compile(model)\n",
    "\n",
    "print(\"Compiling model (this takes 10-30 seconds the first time)...\")\n",
    "with torch.no_grad():\n",
    "    # First call triggers compilation\n",
    "    for _ in range(3):\n",
    "        compiled_model(x)\n",
    "    torch.cuda.synchronize()\n",
    "    compiled_ms = benchmark(lambda: compiled_model(x))\n",
    "\n",
    "# --- Manual kernel patching (same technique as advanced notebook) ---\n",
    "patched_model = LLaMAStyleMLP(hidden_size, intermediate_size).cuda().half()\n",
    "with torch.no_grad():\n",
    "    patched_model.load_state_dict(model.state_dict())\n",
    "\n",
    "norm_weight = patched_model.norm.weight\n",
    "eps = 1e-6\n",
    "gu_proj = patched_model.gate_up_proj\n",
    "d_proj = patched_model.down_proj\n",
    "isize = patched_model.intermediate_size\n",
    "\n",
    "def patched_forward(x):\n",
    "    residual = x\n",
    "    x = norm_kernel.rms_norm_fn(x, norm_weight, None, eps=eps)\n",
    "    gate_up = gu_proj(x)\n",
    "    activated = torch.empty((x.shape[0], isize), dtype=x.dtype, device=x.device)\n",
    "    act_kernel.silu_and_mul(activated, gate_up)\n",
    "    return d_proj(activated) + residual\n",
    "\n",
    "patched_model.forward = patched_forward\n",
    "\n",
    "with torch.no_grad():\n",
    "    patched_ms = benchmark(lambda: patched_model(x))\n",
    "\n",
    "print(f\"\\nLLaMA-style MLP block ({n_tokens} tokens, {hidden_size}→{intermediate_size})\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print()\n",
    "print(f\"  {'Approach':<24} {'Time (ms)':>10} {'Speedup':>8}\")\n",
    "print(f\"  {'='*44}\")\n",
    "print(f\"  {'PyTorch eager':<24} {eager_ms:>8.4f}   {1.0:>6.2f}x\")\n",
    "print(f\"  {'Hub kernel patching':<24} {patched_ms:>8.4f}   {eager_ms/patched_ms:>6.2f}x\")\n",
    "print(f\"  {'torch.compile':<24} {compiled_ms:>8.4f}   {eager_ms/compiled_ms:>6.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is `torch.compile` effective on the full model?\n",
    "\n",
    "`torch.compile` can see the **entire computation graph** and fuse operations that we can't easily fuse with individual kernel replacements. For example, it might fuse the RMSNorm + residual add, or optimize the memory layout for the linear projections. It also uses `cudagraphs` to eliminate kernel launch overhead across the entire forward pass.\n",
    "\n",
    "Manual kernel patching replaces individual operations but can't optimize the **boundaries** between operations the way a whole-graph compiler can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Data Types — fp32 vs fp16 vs bf16\n",
    "\n",
    "Your 4090 supports three floating-point formats. Choosing the right one is a fundamental decision in AI inference:\n",
    "\n",
    "| Type | Bits | Precision | Range | Bandwidth | Use case |\n",
    "|------|------|-----------|-------|-----------|----------|\n",
    "| **fp32** | 32 | ~7 decimal digits | ±3.4×10³⁸ | Baseline | Training (safe default) |\n",
    "| **fp16** | 16 | ~3 decimal digits | ±65,504 | 2x fp32 | Inference (most models) |\n",
    "| **bf16** | 16 | ~3 decimal digits | ±3.4×10³⁸ | 2x fp32 | Training + inference (LLaMA, etc.) |\n",
    "\n",
    "**Key difference between fp16 and bf16:** bf16 has the same *range* as fp32 (it keeps the 8-bit exponent) but less precision (only 7 mantissa bits vs fp16's 10). This means bf16 almost never overflows, making it safer for training. fp16 has better precision but can overflow if values exceed 65,504.\n",
    "\n",
    "**Performance impact:** fp16 and bf16 are half the size of fp32, so they move through memory twice as fast. For memory-bound operations (which most AI ops are), this means ~2x speedup for free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THEORETICAL_BW = 1008  # RTX 4090 GB/s\n",
    "n_tokens = 4096\n",
    "intermediate_size = 11008\n",
    "\n",
    "dtypes = [\n",
    "    (\"fp32\", torch.float32, 4),\n",
    "    (\"fp16\", torch.float16, 2),\n",
    "    (\"bf16\", torch.bfloat16, 2),\n",
    "]\n",
    "\n",
    "print(f\"SiLU + Multiply benchmark across data types\")\n",
    "print(f\"Shape: [{n_tokens}, {intermediate_size}]\")\n",
    "print()\n",
    "print(f\"{'Type':<8} {'Bytes/elem':>10} {'Time (ms)':>10} {'Bandwidth':>12} {'% of peak':>10}\")\n",
    "print(\"-\" * 54)\n",
    "\n",
    "dtype_results = []\n",
    "\n",
    "for name, dtype, bpe in dtypes:\n",
    "    gate = torch.randn((n_tokens, intermediate_size), dtype=dtype, device=\"cuda\")\n",
    "    up = torch.randn_like(gate)\n",
    "\n",
    "    ms = benchmark(lambda: torch.nn.functional.silu(gate) * up)\n",
    "\n",
    "    # Bytes moved: read gate + read up + write silu_out + read silu_out + read up + write result\n",
    "    # (unfused: 6 tensor-sized transfers, but some may be optimized)\n",
    "    # Minimum: read gate + read up + write result = 3 transfers\n",
    "    total_bytes = gate.numel() * bpe * 3  # theoretical minimum for fused\n",
    "    bandwidth = (total_bytes / 1e9) / (ms / 1000)\n",
    "    pct = bandwidth / THEORETICAL_BW * 100\n",
    "\n",
    "    dtype_results.append((name, bpe, ms, bandwidth, pct))\n",
    "    print(f\"{name:<8} {bpe:>10} {ms:>10.4f} {bandwidth:>9.0f} GB/s {pct:>8.0f}%\")\n",
    "\n",
    "print()\n",
    "speedup_16 = dtype_results[0][2] / dtype_results[1][2]\n",
    "print(f\"fp16 is {speedup_16:.1f}x faster than fp32 — almost exactly 2x, because\")\n",
    "print(f\"half the bytes means half the memory traffic.\")\n",
    "print(f\"fp16 and bf16 are the same speed (same size), just different precision/range tradeoff.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the dtype difference\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4.5))\n",
    "\n",
    "names = [r[0] for r in dtype_results]\n",
    "times = [r[2] for r in dtype_results]\n",
    "bandwidths = [r[3] for r in dtype_results]\n",
    "\n",
    "# Left: execution time\n",
    "ax = axes[0]\n",
    "colors = ['#f44336', '#2196F3', '#4CAF50']\n",
    "bars = ax.bar(names, times, color=colors)\n",
    "ax.set_ylabel('Time (ms) — lower is better')\n",
    "ax.set_title('SiLU + Multiply: Execution Time by Data Type')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "for bar, t in zip(bars, times):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
    "            f'{t:.3f}ms', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Right: bandwidth utilization\n",
    "ax = axes[1]\n",
    "bars = ax.bar(names, bandwidths, color=colors)\n",
    "ax.axhline(y=THEORETICAL_BW, color='red', linestyle='--', linewidth=1, label=f'4090 peak ({THEORETICAL_BW} GB/s)')\n",
    "ax.set_ylabel('Achieved Bandwidth (GB/s)')\n",
    "ax.set_title('Memory Bandwidth Utilization by Data Type')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "for bar, bw in zip(bars, bandwidths):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5,\n",
    "            f'{bw:.0f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"All three types achieve similar bandwidth utilization (~55% of peak).\")\n",
    "print(\"But fp16/bf16 move half as many bytes, so they finish in half the time.\")\n",
    "print(\"The GPU memory bus is the bottleneck — smaller data types = faster.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision vs. Speed: Does the accuracy loss matter?\n",
    "\n",
    "Let's see how much numerical error each data type introduces compared to fp32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test data in fp32 (ground truth)\n",
    "torch.manual_seed(42)\n",
    "x_fp32 = torch.randn((1024, 4096), dtype=torch.float32, device=\"cuda\")\n",
    "w_fp32 = torch.randn(4096, dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "# Compute RMSNorm in fp32 (\"perfect\" answer)\n",
    "variance = x_fp32.pow(2).mean(dim=-1, keepdim=True)\n",
    "ref = x_fp32 * torch.rsqrt(variance + 1e-6) * w_fp32\n",
    "\n",
    "print(f\"{'Type':<8} {'Max error':>12} {'Mean error':>12} {'Relative':>12}\")\n",
    "print(\"-\" * 48)\n",
    "\n",
    "for name, dtype in [(\"fp32\", torch.float32), (\"fp16\", torch.float16), (\"bf16\", torch.bfloat16)]:\n",
    "    x = x_fp32.to(dtype)\n",
    "    w = w_fp32.to(dtype)\n",
    "\n",
    "    variance = x.float().pow(2).mean(dim=-1, keepdim=True)\n",
    "    result = (x.float() * torch.rsqrt(variance + 1e-6) * w.float()).to(dtype).float()\n",
    "\n",
    "    abs_err = (result - ref).abs()\n",
    "    rel_err = abs_err / (ref.abs() + 1e-8)\n",
    "\n",
    "    print(f\"{name:<8} {abs_err.max().item():>12.6f} {abs_err.mean().item():>12.8f} {rel_err.mean().item():>12.8f}\")\n",
    "\n",
    "print()\n",
    "print(\"fp16 and bf16 introduce small errors (~0.001) that are invisible\")\n",
    "print(\"in model outputs. Modern models are trained to be robust to this.\")\n",
    "print(\"bf16 has slightly more error (fewer mantissa bits) but never overflows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: Memory Footprint — Fusion Saves RAM, Not Just Time\n",
    "\n",
    "Kernel fusion doesn't just make things faster — it reduces **peak GPU memory usage**. This matters because GPU memory is the most constrained resource in AI inference. If your model barely fits in 24 GB of VRAM, eliminating intermediate tensors could be the difference between running and crashing with an OOM error.\n",
    "\n",
    "Let's measure exactly how much memory each approach uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_peak_memory(func):\n",
    "    \"\"\"Run a function and return peak GPU memory allocated in MB.\"\"\"\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    torch.cuda.synchronize()\n",
    "    baseline = torch.cuda.memory_allocated()\n",
    "    func()\n",
    "    torch.cuda.synchronize()\n",
    "    peak = torch.cuda.max_memory_allocated()\n",
    "    return (peak - baseline) / 1e6  # MB above baseline\n",
    "\n",
    "\n",
    "n_tokens = 8192\n",
    "inter = 11008\n",
    "\n",
    "# Pre-allocate inputs outside the measurement\n",
    "gate = torch.randn((n_tokens, inter), dtype=torch.float16, device=\"cuda\")\n",
    "up = torch.randn((n_tokens, inter), dtype=torch.float16, device=\"cuda\")\n",
    "gate_up_cat = torch.cat([gate, up], dim=-1).contiguous()\n",
    "\n",
    "print(f\"Peak memory for SiLU + Multiply ({n_tokens} x {inter}, float16)\")\n",
    "print(f\"Input data: {gate.numel() * 2 * 2 / 1e6:.0f} MB (gate + up)\")\n",
    "print()\n",
    "\n",
    "# Unfused: PyTorch creates an intermediate tensor for silu(gate)\n",
    "mem_unfused = measure_peak_memory(\n",
    "    lambda: torch.nn.functional.silu(gate) * up\n",
    ")\n",
    "\n",
    "# Fused: Hub kernel writes directly to output, no intermediate\n",
    "out_hub = torch.empty((n_tokens, inter), dtype=torch.float16, device=\"cuda\")\n",
    "mem_fused = measure_peak_memory(\n",
    "    lambda: act_kernel.silu_and_mul(out_hub, gate_up_cat)\n",
    ")\n",
    "\n",
    "# Our Triton kernel\n",
    "mem_triton = measure_peak_memory(\n",
    "    lambda: triton_silu_mul(gate, up)\n",
    ")\n",
    "\n",
    "intermediate_size_mb = n_tokens * inter * 2 / 1e6  # the silu(gate) intermediate\n",
    "\n",
    "print(f\"  {'Approach':<20} {'Peak memory':>14} {'Overhead':>14}\")\n",
    "print(f\"  {'='*50}\")\n",
    "print(f\"  {'PyTorch (unfused)':<20} {mem_unfused:>10.0f} MB   {mem_unfused - mem_fused:>+10.0f} MB\")\n",
    "print(f\"  {'Our Triton kernel':<20} {mem_triton:>10.0f} MB   {mem_triton - mem_fused:>+10.0f} MB\")\n",
    "print(f\"  {'Hub kernel (fused)':<20} {mem_fused:>10.0f} MB   {'(baseline)':>14}\")\n",
    "print()\n",
    "print(f\"  The unfused version allocates ~{intermediate_size_mb:.0f} MB for the intermediate\")\n",
    "print(f\"  silu(gate) tensor. Fused kernels skip this entirely.\")\n",
    "print()\n",
    "print(f\"  In a 32-layer model, that's {intermediate_size_mb * 32 / 1e3:.1f} GB of peak memory savings —\")\n",
    "print(f\"  potentially enough to fit a larger model or batch in your 24 GB of VRAM.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full model memory comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare memory usage across all data types for a full MLP forward pass\n",
    "n_tokens = 4096\n",
    "hidden_size = 4096\n",
    "intermediate_size = 11008\n",
    "\n",
    "print(f\"MLP block peak memory by data type ({n_tokens} tokens)\")\n",
    "print(f\"{'Type':<8} {'Peak alloc':>12} {'Model size':>12} {'Activation':>12}\")\n",
    "print(\"-\" * 48)\n",
    "\n",
    "mem_results = []\n",
    "\n",
    "for name, dtype in [(\"fp32\", torch.float32), (\"fp16\", torch.float16), (\"bf16\", torch.bfloat16)]:\n",
    "    mdl = LLaMAStyleMLP(hidden_size, intermediate_size).to(device=\"cuda\", dtype=dtype)\n",
    "    x = torch.randn((n_tokens, hidden_size), dtype=dtype, device=\"cuda\")\n",
    "\n",
    "    model_bytes = sum(p.numel() * p.element_size() for p in mdl.parameters())\n",
    "\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    torch.cuda.synchronize()\n",
    "    before = torch.cuda.memory_allocated()\n",
    "    with torch.no_grad():\n",
    "        out = mdl(x)\n",
    "    torch.cuda.synchronize()\n",
    "    peak = torch.cuda.max_memory_allocated() - before\n",
    "\n",
    "    activation_mem = peak - model_bytes\n",
    "    mem_results.append((name, peak / 1e6, model_bytes / 1e6, max(0, activation_mem) / 1e6))\n",
    "    print(f\"{name:<8} {peak/1e6:>9.0f} MB  {model_bytes/1e6:>9.0f} MB  {max(0,activation_mem)/1e6:>9.0f} MB\")\n",
    "\n",
    "    del mdl, x, out\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print()\n",
    "print(f\"fp32→fp16 cuts memory roughly in half. This is why fp16/bf16 inference\")\n",
    "print(f\"is standard — it's not just faster, it lets you fit larger models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 4.5))\n",
    "\n",
    "names = [r[0] for r in mem_results]\n",
    "model_mem = [r[2] for r in mem_results]\n",
    "act_mem = [r[3] for r in mem_results]\n",
    "\n",
    "x_pos = range(len(names))\n",
    "bars1 = ax.bar(x_pos, model_mem, label='Model weights', color='#2196F3')\n",
    "bars2 = ax.bar(x_pos, act_mem, bottom=model_mem, label='Activations (intermediate)', color='#FF9800')\n",
    "\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(names)\n",
    "ax.set_ylabel('GPU Memory (MB)')\n",
    "ax.set_title('MLP Block Memory Breakdown by Data Type')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for i, (m, a) in enumerate(zip(model_mem, act_mem)):\n",
    "    ax.text(i, m + a + 5, f'{m+a:.0f} MB', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 7: When to Use What — A Decision Framework\n",
    "\n",
    "After three notebooks of benchmarks, here's the practical summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comprehensive comparison\n",
    "n_tokens = 4096\n",
    "gate = torch.randn((n_tokens, intermediate_size), dtype=torch.float16, device=\"cuda\")\n",
    "up = torch.randn((n_tokens, intermediate_size), dtype=torch.float16, device=\"cuda\")\n",
    "gate_up_cat = torch.cat([gate, up], dim=-1).contiguous()\n",
    "out_hub = torch.empty_like(gate)\n",
    "\n",
    "@torch.compile\n",
    "def compiled_fn(g, u):\n",
    "    return torch.nn.functional.silu(g) * u\n",
    "\n",
    "# Warm up compile\n",
    "for _ in range(5): compiled_fn(gate, up)\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "approaches = [\n",
    "    (\"PyTorch eager\",    lambda: torch.nn.functional.silu(gate) * up,     \"Zero\",            \"None\"),\n",
    "    (\"torch.compile\",    lambda: compiled_fn(gate, up),                    \"1 decorator\",     \"Slow first call\"),\n",
    "    (\"Hub kernel\",       lambda: act_kernel.silu_and_mul(out_hub, gate_up_cat), \"1 line\",     \"Hub must have it\"),\n",
    "    (\"Custom Triton\",    lambda: triton_silu_mul(gate, up),                \"~15 lines\",       \"Full control\"),\n",
    "]\n",
    "\n",
    "print(f\"Final comparison: SiLU + Multiply, {n_tokens} x {intermediate_size}, float16\")\n",
    "print()\n",
    "print(f\"{'Approach':<20} {'Time (ms)':>10} {'Speedup':>8} {'Effort':<16} {'Tradeoff'}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "base_ms = None\n",
    "for name, func, effort, tradeoff in approaches:\n",
    "    ms = benchmark(func)\n",
    "    if base_ms is None:\n",
    "        base_ms = ms\n",
    "    print(f\"{name:<20} {ms:>8.4f}   {base_ms/ms:>6.2f}x   {effort:<16} {tradeoff}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What we covered in this notebook\n",
    "\n",
    "**Writing GPU kernels with Triton:** You wrote two kernels from scratch — a simple element-wise fused activation, and a more complex RMSNorm with a reduction. Triton gives you Python-like syntax that compiles to GPU machine code.\n",
    "\n",
    "**`torch.compile`:** PyTorch's built-in compiler auto-fuses operations with zero manual effort. It matches custom kernel performance for standard operations. The cost is a slow first call (compilation) and less control over what gets fused.\n",
    "\n",
    "**The four-way race showed convergence:** Custom Triton, Hub kernels, and `torch.compile` all hit the same memory bandwidth ceiling. Once you fuse operations, there's no magic left — you're limited by how fast the 4090 can move bytes.\n",
    "\n",
    "**Data types matter as much as kernel optimization:** Switching from fp32 to fp16/bf16 gives a reliable 2x speedup *and* halves memory usage. This is often the single biggest optimization you can make.\n",
    "\n",
    "**Fusion saves memory, not just time:** Eliminating intermediate tensors reduces peak VRAM. Across a 32-layer model, this can free gigabytes — potentially the difference between a model fitting on your GPU or not.\n",
    "\n",
    "### When to use what\n",
    "\n",
    "| Situation | Recommendation |\n",
    "|-----------|---------------|\n",
    "| Quick prototype, standard ops | `torch.compile` |\n",
    "| Production inference server | Hub kernels (battle-tested, no compile overhead) |\n",
    "| Custom operation not in PyTorch | Write a Triton kernel |\n",
    "| Maximum performance, willing to invest time | Custom CUDA C++ |\n",
    "| Not sure yet | Start with `torch.compile`, profile, replace bottlenecks |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
